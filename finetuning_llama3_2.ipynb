{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPI3dEQBv8fKIUmcha3z5/Z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PsyCharan17/pytorch-adventures/blob/main/finetuning_llama3_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S898Oo8QiNUF"
      },
      "outputs": [],
      "source": [
        "pip install unsloth transformers trl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "from datasets import load_dataset\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "from unsloth.chat_templates import get_chat_template , standardize_sharegpt"
      ],
      "metadata": {
        "id": "m-Vta4Sun6K5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model , tokenizer = FastLanguageModel.from_pretrained(model_name=\"unsloth/Llama-3.2-3B-Instruct\",\n",
        "                                                      max_seq_length=2048,\n",
        "                                                      load_in_4bit=True)\n"
      ],
      "metadata": {
        "id": "wDd_RpApoz6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = FastLanguageModel.get_peft_model(model,r=16,\n",
        "                                         target_modules = [\"q_proj\" , \"k_proj\" , \"v_proj\" , \"o_proj\",\"gate_proj\" ,\"up_proj\",\"down_proj\"],\n",
        "                                         )\n"
      ],
      "metadata": {
        "id": "9m991BVHo94q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_chat_template(tokenizer,chat_template=\"llama-3.1\")"
      ],
      "metadata": {
        "id": "WK5leTtCo-BY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(tokenizer)"
      ],
      "metadata": {
        "id": "jr1gWXbLu_PK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"mlabonne/FineTome-100k\", split=\"train\")"
      ],
      "metadata": {
        "id": "cRJA89vYo-J3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = standardize_sharegpt(dataset)"
      ],
      "metadata": {
        "id": "KvT7G9LdscuB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[1]"
      ],
      "metadata": {
        "id": "RP56dVFzo-RP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(\n",
        "    lambda examples: {\n",
        "        \"text\": [\n",
        "            tokenizer.apply_chat_template(convo, tokenize=False)\n",
        "            for convo in examples[\"conversations\"]\n",
        "\n",
        "        ]\n",
        "    },\n",
        "  batched=True,\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "C35jawMTo-ap"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "53zxuL7To-hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0][\"text\"]"
      ],
      "metadata": {
        "id": "Fzz0dPq-o-oo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    train_dataset = dataset,\n",
        "    dataset_text_field = \"text\",\n",
        "    max_seq_length = 2048,\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=2,\n",
        "        gradient_accumulation_steps=4,\n",
        "        warmup_steps=5,\n",
        "        max_steps=60,\n",
        "        learning_rate=2e-4,\n",
        "        fp16=not torch.cuda.is_bf16_supported(),\n",
        "        bf16 = torch.cuda.is_bf16_supported(),\n",
        "        output_dir=\"outputs\",\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "id": "1f_ZiHjHtW7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "hdXhcv8btxY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"finetuned_model\")"
      ],
      "metadata": {
        "id": "1ujJQPfBtxf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inference_model , inference_tokenizer = FastLanguageModel.from_pretrained(model_name=\"./finetuned_model\",\n",
        "                                                      max_seq_length=2048,\n",
        "                                                      load_in_4bit=True)"
      ],
      "metadata": {
        "id": "-Mq8iDS8txn3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "yLzlxC2XFX2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_prompts = [\n",
        "    \"Why does GPT2 use a Conv1D with kernel size of 1 as its projection layer?\"\n",
        "]\n",
        "\n",
        "for prompt in text_prompts:\n",
        "  formatted_prompt = inference_tokenizer.apply_chat_template([{\n",
        "      \"role\": \"user\",\n",
        "      \"content\": prompt\n",
        "  }], tokenize=False)\n",
        "\n",
        "  model_inputs = inference_tokenizer(formatted_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "  generated_ids = inference_model.generate(\n",
        "      **model_inputs,\n",
        "      max_new_tokens=512,\n",
        "      temperature=0.7,\n",
        "      do_sample=True,\n",
        "      pad_token_id=inference_tokenizer.pad_token_id\n",
        "\n",
        "  )\n",
        "\n",
        "  response = inference_tokenizer.batch_decode(generated_ids,skip_special_tokens=True)[0]\n",
        "  print(response)\n",
        "\n"
      ],
      "metadata": {
        "id": "f6MFnrMKtxuS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Tl7UXm0OF0cI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9DY3yo3F0kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cqyZnpgwF0rq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}